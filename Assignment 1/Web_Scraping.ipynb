{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Web Scraping.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanda92/CE706-Information-Retrieval/blob/master/Assignment%201/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2mT0IGmlnOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the Natural Language Toolkit\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTYwY4l3lnOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Section 1: Engineering system to read urls \n",
        "url = 'http://www.multimediaeval.org/mediaeval2019/memorability/'\n",
        "url2 = 'http://sites.google.com/view/siirh2020/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkAUfN7glnOQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "72ff591a-ec30-4c4f-dfb0-8a8e216c9f05"
      },
      "source": [
        "import requests\n",
        "raw = requests.get(url)\n",
        "raw2 = requests.get(url2)\n",
        "type(raw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "requests.models.Response"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qihy7-NclnOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Section 2: HTML Parsing\n",
        "html = raw.text\n",
        "html2 = raw2.text\n",
        "#print(html)\n",
        "#print(html2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9l3LV8wlnOe",
        "colab_type": "code",
        "colab": {},
        "outputId": "9af4e28c-1233-4712-8660-dca726d1a043"
      },
      "source": [
        "# Import BeautifulSoup from bs4.\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Create a BeautifulSoup objects from the html texts\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "soup2 = BeautifulSoup(html2, \"html.parser\")\n",
        "\n",
        "type(soup)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoMQaWXGlnOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting text\n",
        "text = soup.get_text()\n",
        "text2 = soup2.get_text()\n",
        "# type(text)\n",
        "# print(text)\n",
        "# print(text2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBm7J5q8lnOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining function to add path to save data in to txt file after each stage of processing\n",
        "\n",
        "folder_path = \"C:/Users/lenovo/Documents/Information Retrieval/Assignment 1/Output Files\"\n",
        "def write_file(filename,contents):\n",
        "    file_path = folder_path + \"/\" + filename + \".txt\"\n",
        "    file= open(file_path,\"w\",encoding=\"utf-8\")\n",
        "    file.write(contents) \n",
        "    file.close()\n",
        "    \n",
        "write_file(\"text\",text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZcGY5UlnOy",
        "colab_type": "code",
        "colab": {},
        "outputId": "d3f6b447-da50-41ce-9e14-13331f644d58"
      },
      "source": [
        "#Section 3: Pre-processing: Sentence Splitting, Tokenization and Normalization \n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "# Section 3.1: tokenize into sentences\n",
        "sentences_tokens = sent_tokenize(text)\n",
        "sentences_tokens2 = sent_tokenize(text2)\n",
        "\n",
        "# type(sentences_tokens)\n",
        "# print(sentences_tokens)\n",
        "# print(sentences_tokens2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N73XRhJ1lnO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Section 3.2: sentence splitting, separated by a space\n",
        "sentences_tokens_str =' '.join( sentences_tokens )\n",
        "sentences_tokens_str2 =' '.join( sentences_tokens2 )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"sentences_tokens_str\",sentences_tokens_str)\n",
        "write_file(\"sentences_tokens_str2\",sentences_tokens_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAKf6oB1lnO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize into words.\n",
        "words_tokens = word_tokenize(text)\n",
        "words_tokens2 = word_tokenize(text2)\n",
        "# type(words_tokens)\n",
        "# print(words_tokens)\n",
        "# print(words_tokens2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RkZI7anlnPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#words splitting, separated by a space\n",
        "words_tokens_str =' '.join( words_tokens )\n",
        "words_tokens_str2 =' '.join( words_tokens2 )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"words_tokens_str\",words_tokens_str)\n",
        "write_file(\"words_tokens_str2\",words_tokens_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1gGcLpplnPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Section 3.3: Normalization\n",
        "# Normalising upper case letters in to lowercase\n",
        "\n",
        "length= len(words_tokens)\n",
        "y=words_tokens\n",
        "type(y)\n",
        "for i in range(length):\n",
        "    y[i] = words_tokens[i].lower()\n",
        "    \n",
        "text_lower = y\n",
        "\n",
        "# type(text_lower)\n",
        "# print(text_lower)\n",
        "\n",
        "\n",
        "length= len(words_tokens2)\n",
        "y=words_tokens2\n",
        "type(y)\n",
        "for i in range(length):\n",
        "    y[i] = words_tokens2[i].lower()\n",
        "    \n",
        "text_lower2 = y\n",
        "\n",
        "# type(text_lower)\n",
        "# print(text_lower)\n",
        "# print(text_lower2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK_XqpoDlnPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting texts, separated by a space.\n",
        "text_lower_str =' '.join( text_lower )\n",
        "text_lower_str2 =' '.join( text_lower2 )\n",
        "\n",
        "# Write to a file\n",
        "write_file(\"text_lower_str\",text_lower_str)\n",
        "write_file(\"text_lower_str2\",text_lower_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaKz6FO8lnPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove punctuation from the text.\n",
        "\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~+=`|'''\n",
        "\n",
        "y=text_lower\n",
        "length=len(text_lower)\n",
        "\n",
        "for i in range(length):\n",
        "    no_punct = \"\"\n",
        "    for char in text_lower[i]:\n",
        "        \n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "    y[i]=no_punct\n",
        "\n",
        "# Remove empty elements from the list.\n",
        "\n",
        "z=[]\n",
        "for i in range(length):\n",
        "    if y[i] != '':\n",
        "        z.append(y[i])   \n",
        "    \n",
        "text_remove_punc = z\n",
        "   \n",
        "# print(text_remove_punc)\n",
        "\n",
        "y=text_lower2\n",
        "length=len(text_lower2)\n",
        "\n",
        "for i in range(length):\n",
        "    no_punct2 = \"\"\n",
        "    for char in text_lower2[i]:\n",
        "        \n",
        "        if char not in punctuations:\n",
        "            no_punct2 = no_punct2 + char\n",
        "    y[i]=no_punct2\n",
        "\n",
        "# Remove empty elements from the list.\n",
        "\n",
        "z=[]\n",
        "for i in range(length):\n",
        "    if y[i] != '':\n",
        "        z.append(y[i])   \n",
        "    \n",
        "text_remove_punc2 = z\n",
        "   \n",
        "# print(text_remove_punc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlt8FsrslnPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting text after removal of punctuation, separated by a space.\n",
        "text_remove_punc_str =' '.join( text_remove_punc )\n",
        "text_remove_punc_str2 =' '.join( text_remove_punc2 )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"text_remove_punc_str\",text_remove_punc_str)\n",
        "write_file(\"text_remove_punc_str2\",text_remove_punc_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX18UIDGlnPe",
        "colab_type": "code",
        "colab": {},
        "outputId": "a2d348a6-46ab-4499-b428-f2e3e90f5ca1"
      },
      "source": [
        "# Section 4: Selecting Keywords\n",
        "\n",
        "import nltk\n",
        "# Downloading nltk stopwords list to filter unnecessary words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "filtered_words = [word for word in text_remove_punc if word not in stopwords.words('english')]\n",
        "filtered_words2 = [word for word in text_remove_punc2 if word not in stopwords.words('english')]\n",
        "# print(filtered_words)\n",
        "# type(filtered_words)\n",
        "# print(filtered_words2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md_WeBy_lnPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting filtered words further, separated by a space\n",
        "filtered_words_str =' '.join( filtered_words )\n",
        "filtered_words_str2 =' '.join( filtered_words2 )\n",
        "\n",
        "# Write to a file\n",
        "write_file(\"filtered_words_str\",filtered_words_str)\n",
        "write_file(\"filtered_words_str2\",filtered_words_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgvNtwazlnPs",
        "colab_type": "code",
        "colab": {},
        "outputId": "044dfaf5-c06e-44f1-a44c-a99c8ef63ca2"
      },
      "source": [
        "#Section 4: Part of speech tagging\n",
        "# Downloading nltk average perceptron tagger\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text_pos = nltk.pos_tag(filtered_words)\n",
        "# print(text_pos)\n",
        "\n",
        "text_pos2 = nltk.pos_tag(filtered_words2)\n",
        "# print(text_pos2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhCwW3GhlnPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the list of tuples (containing two strings) into a list.\n",
        "\n",
        "length=len(text_pos)\n",
        "z=[]\n",
        "for i in range(length):\n",
        "    z.append(text_pos[i][0])\n",
        "    z.append(text_pos[i][1])\n",
        "    \n",
        "# Convert the list of strings to a string object, separated by a space.\n",
        "text_pos_str =' '.join( z )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"text_pos_str\",text_pos_str)    \n",
        "\n",
        "# Convert the list of tuples (containing two strings) into a list.\n",
        "\n",
        "length=len(text_pos2)\n",
        "z=[]\n",
        "for i in range(length):\n",
        "    z.append(text_pos2[i][0])\n",
        "    z.append(text_pos2[i][1])\n",
        "    \n",
        "# Convert the list of strings to a string object, separated by a space.\n",
        "text_pos_str2 =' '.join( z )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"text_pos_str2\",text_pos_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abKuhu31lnP1",
        "colab_type": "code",
        "colab": {},
        "outputId": "d407c380-bf57-4cf3-a29b-d216c17ccfee"
      },
      "source": [
        "#Section 6: Stemming or Morphological Analysis \n",
        "# 'Lemmatisation  is the process of grouping together the inflected forms of a word so they can be analysed as a single item,\n",
        "#  identified by the word's lemma, or dictionary form' - Wikipedia.\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatizer_list = []\n",
        "for token, tag in text_pos:\n",
        "    lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
        "    # print(type(lemma))\n",
        "    # print(token, \"=>\", lemma)\n",
        "    lemmatizer_list.append(lemma)\n",
        "# print(lemmatizer_list)\n",
        "\n",
        "lemmatizer2 = WordNetLemmatizer()\n",
        "\n",
        "lemmatizer_list2 = []\n",
        "for token2, tag2 in text_pos2:\n",
        "    lemma2 = lemmatizer2.lemmatize(token2, tag_map[tag2[0]])\n",
        "    # print(type(lemma))\n",
        "    # print(token, \"=>\", lemma)\n",
        "    lemmatizer_list2.append(lemma2)\n",
        "\n",
        "# print(lemmatizer_list)\n",
        "# print(lemmatizer_list2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx819_XDlnP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting lemmatized words, separated by a space.\n",
        "lemmatizer_list_str =' '.join( lemmatizer_list )\n",
        "lemmatizer_list_str2 =' '.join( lemmatizer_list2 )\n",
        "\n",
        "# Write to a file.\n",
        "write_file(\"lemmatizer_list_str\",lemmatizer_list_str)\n",
        "write_file(\"lemmatizer_list_str2\",lemmatizer_list_str2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFexQQ7flnP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Section 7: tf*idf\n",
        "doc1 =lemmatizer_list\n",
        "doc1_set = list(set(doc1))\n",
        "# print(doc1_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9WqXNiJlnQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc2 =lemmatizer_list2\n",
        "doc2_set = list(set(doc2))\n",
        "# print(doc2_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S3n75C8lnQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count the number of times each word appears in the document.\n",
        "length = len(doc1_set)\n",
        "doc1_count = []\n",
        "for i in range(length):\n",
        "    # print(doc1_set[i], doc1.count(doc1_set[i]))\n",
        "    doc1_count.append(doc1.count(doc1_set[i]))\n",
        "# print(doc1_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed9UGRdXlnQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count the number of times each word appears in the document.\n",
        "length = len(doc2_set)\n",
        "doc2_count = []\n",
        "for i in range(length):\n",
        "    # print(doc2_set[i], doc2.count(doc2_set[i]))\n",
        "    doc2_count.append(doc2.count(doc2_set[i]))\n",
        "# print(doc2_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAl7aSbFlnQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use pandas dataframes to store the information.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df1=pd.DataFrame(doc1_set,columns=['word'])\n",
        "\n",
        "# Note: 'tf' stands for term frequency - the number of times a term appears in a document.\n",
        "df1['tf_count']=doc1_count\n",
        "# print(df1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4f1DNOBlnQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=pd.DataFrame(doc2_set,columns=['word'])\n",
        "# Note: 'tf' stands for term frequency - the number of times a term appears in a document.\n",
        "df2['tf_count']=doc2_count\n",
        "# print(df2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao1FD9BYlnQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the two sets of words.\n",
        "doc_combined =doc1_set+doc2_set\n",
        "# print(doc_combined)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkDnffN5lnQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure the elements are distinct.\n",
        "doc_combined_set = list(set(doc_combined))\n",
        "# print(doc_combined_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ85YSpelnQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe for the combined sets of words.\n",
        "dfc =pd.DataFrame(doc_combined_set,columns=['word'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_JYB7HylnQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = len(doc_combined_set)\n",
        "\n",
        "x=[]\n",
        "for i in range(length):\n",
        "    tf_count=0\n",
        "    if dfc['word'][i] in doc1_set:\n",
        "        index1=doc1_set.index(dfc['word'][i])\n",
        "        tf_count = tf_count + df1['tf_count'][index1]\n",
        "    x.append(tf_count)\n",
        "\n",
        "dfc['tf_doc1']=x    \n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdQ5M4XVlnQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = len(doc_combined_set)\n",
        "\n",
        "x=[]\n",
        "for i in range(length):\n",
        "    tf_count=0\n",
        "    if dfc['word'][i] in doc2_set:\n",
        "        index2=doc2_set.index(dfc['word'][i])\n",
        "        tf_count = tf_count + df2['tf_count'][index2]\n",
        "    x.append(tf_count)\n",
        "\n",
        "dfc['tf_doc2']=x    \n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYefMmrelnQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the number of times a term appears over all documents.\n",
        "# This value is not used in subsequent calculations!\n",
        "\n",
        "length = len(doc_combined_set)\n",
        "x=[]\n",
        "for i in range(length):\n",
        "    tf_count = 0\n",
        "    if dfc['word'][i] in doc1_set:\n",
        "        index1=doc1_set.index(dfc['word'][i])\n",
        "        #print(index1)\n",
        "        #print(df1['count'][index1])\n",
        "        tf_count = tf_count + df1['tf_count'][index1]\n",
        "    if dfc['word'][i] in doc2_set:\n",
        "        index2=doc2_set.index(dfc['word'][i])\n",
        "        #print(index2)\n",
        "        #print(df2['count'][index2])\n",
        "        tf_count = tf_count + df2['tf_count'][index2]\n",
        "    x.append(tf_count)\n",
        "        \n",
        "dfc['tf_count']=x      \n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89yFT_CNlnQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate df_t - the number of documents a term t appears. \n",
        "\n",
        "length = len(doc_combined_set)\n",
        "\n",
        "x=[]\n",
        "for i in range(length):\n",
        "    j=0\n",
        "    if dfc['word'][i] in doc1_set:\n",
        "        j = j+1\n",
        "    if dfc['word'][i] in doc2_set:\n",
        "        j = j+1   \n",
        "    x.append(j)\n",
        "\n",
        "dfc['df_t']=x    \n",
        "\n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoCP2tpwlnQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate idf - this is log(N)/df_t (log to any base you like!).\n",
        "# The default is the naural logarithm.\n",
        "import math\n",
        "\n",
        "length = len(doc_combined_set)\n",
        "\n",
        "N=2 # Total number of documents\n",
        "x=[]\n",
        "for i in range(length):\n",
        "    y=N/(dfc['df_t'][i]) \n",
        "    z = math.log(y)\n",
        "    x.append(z)\n",
        "\n",
        "dfc['idf']=x    \n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpdDJiSPlnRB",
        "colab_type": "code",
        "colab": {},
        "outputId": "b48ec2f8-096d-4865-ad4a-fbb2cfb5112f"
      },
      "source": [
        "# Finally, calculate tf.idf. \n",
        "\n",
        "length = len(doc_combined_set)\n",
        "print(length)\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "for i in range(length):\n",
        "    y1 =(dfc['tf_doc1'][i])*(dfc['idf'][i])\n",
        "    x1.append(y1)\n",
        "    y2 =(dfc['tf_doc2'][i])*(dfc['idf'][i])\n",
        "    x2.append(y2)\n",
        "\n",
        "dfc['tf_idf_doc1']=x1  \n",
        "dfc['tf_idf_doc2']=x2  \n",
        "# print(dfc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObTOVF2WlnRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort in descending order with respect to the tf.idf.\n",
        "\n",
        "sorted_dfc1 = dfc.sort_values(by='tf_idf_doc1',ascending=False)\n",
        "sorted_dfc2 = dfc.sort_values(by='tf_idf_doc2',ascending=False)\n",
        "#print(sorted_dfc1)\n",
        "#print(sorted_dfc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ2fo5ZDlnRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the words with greatest tf.idf to a file, for each document\n",
        "\n",
        "# print(sorted_dfc1.index)\n",
        "# type(sorted_dfc1.index)\n",
        "\n",
        "indices1=list(sorted_dfc1.index)\n",
        "indices2=list(sorted_dfc2.index)\n",
        "#print(indices1)\n",
        "#print(indices1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9c6c3YHlnRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the 300 most relevant words to the index file. \n",
        "\n",
        "Most_relevant = 300\n",
        "x=[]\n",
        "for i in range(0, Most_relevant):\n",
        "    #print(sorted_dfc1['word'][indices1[i]],sorted_dfc1['tf_idf_doc1'][indices1[i]])\n",
        "    x.append(sorted_dfc1['word'][indices1[i]])\n",
        "    x.append(str(sorted_dfc1['tf_idf_doc1'][indices1[i]]))\n",
        "index_doc1 = x\n",
        "# print(x)\n",
        "\n",
        "# Convert the list of strings to a string object, separated by a space\n",
        "index_doc1_str =' '.join( index_doc1 )\n",
        "\n",
        "# Write to a file\n",
        "write_file(\"index_doc1_str\",index_doc1_str)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FySBabQUlnRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Most_relevant = 300\n",
        "x=[]\n",
        "for i in range(0, Most_relevant):\n",
        "    #print(sorted_dfc2['word'][indices2[i]],sorted_dfc2['tf_idf_doc2'][indices2[i]])\n",
        "    x.append(sorted_dfc2['word'][indices2[i]])\n",
        "    x.append(str(sorted_dfc2['tf_idf_doc2'][indices2[i]]))\n",
        "index_doc2 = x\n",
        "\n",
        "# Convert the list of strings to a string object, separated by a space\n",
        "index_doc2_str =' '.join( index_doc2 )\n",
        "\n",
        "# Write to a file\n",
        "write_file(\"index_doc2_str\",index_doc2_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ7kwSU-lnRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}